## 梯度下降法

* 不是一个机器学习算法
* 是一种基于搜索的最优化方法
* 作用：最小化一个损失函数
* 梯度上升法：最大化一个效用函数

$$
-\eta\frac{dJ}{d\theta}
$$

* $\eta$称为学习率(learning rate)
* $\eta$的取值影响获得最优解的速度
* $\eta$取值不合适，甚至得不到最优解
* $\eta$是梯度下降法的一个超参数
* $\eta$太小，减慢收敛学习速度
* $\eta$太大，甚至导致不收敛
* 并不是所有函数都有唯一的极值点
  - 多次运行，随机化初始点
  - 梯度下降法的初始点也是一个超参数

#### 线性回归中使用梯度下降法

目标：使$\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$尽可能小 线性回归的损失函数具有唯一的最优解
$$
\hat{y}^{(i)} = \theta_{0}+\theta_{1}X_{1}^{(i)}+\theta_{2}X_{2}^{(i)}+...+\theta_{n}X_{n}^{(i)} \\
\sum_{i=1}^m(y^{(i)}-\theta_{0}-\theta_{1}X_{1}^{(i)}-\theta_{2}X_{2}^{(i)} -...-\theta_{n}X_{n}^{(i)})^2
$$

#### 多元线性回归中的梯度下降法

导数可以代表方向，对应J增大的方向

$-\eta\frac{dJ}{d\theta} \qquad \theta=(\theta_{0},\theta_{1},…,\theta_{n})$
$$
-\eta\frac{dJ}{d\theta} = -\eta\nabla J \qquad \nabla J = (\frac{\partial{J}}{\partial{\theta_{0}}}, \frac{\partial{J}}{\partial{\theta_{1}}},\frac{\partial{J}}{\partial{\theta_{2}}},...,\frac{\partial{J}}{\partial{\theta_{n}}})
$$

$$
\nabla{J(\theta)}=\begin{pmatrix}
\frac{\partial{J}}{\partial\theta_{0}} \\\\
\frac{\partial{J}}{\partial\theta_{1}} \\\\
\frac{\partial{J}}{\partial\theta_{2}} \\\\
\cdots \\\\
\frac{\partial{J}}{\partial\theta_{n}}
\end{pmatrix}  
= \begin{pmatrix}
\sum_{i=1}^m2(y^{(i)}-X_{b}^{(i)}\theta).(-1) \\\\
\sum_{i=1}^m2(y^{(i)}-X_{b}^{(i)}\theta).(-X_{1}^{(i)}) \\\\
\sum_{i=1}^m2(y^{(i)}-X_{b}^{(i)}\theta).(-X_{2}^{(i)}) \\\\
\cdots \\\\
\sum_{i=1}^m2(y^{(i)}-X_{b}^{(i)}\theta).(-X_{n}^{(i)}) \\\\
\end{pmatrix} 
=2.\begin{pmatrix}
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}) \\\\
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{1}^{(i)} \\\\
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{2}^{(i)} \\\\
\cdots \\\\
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{n}^{(i)} \\\\
\end{pmatrix}
$$

目标：使$\frac{1}{m}\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$尽可能小 $J{\theta}=MSE(y,\hat{y})$
$$
\nabla{J(\theta)}=\frac{2}{m}.\begin{pmatrix}
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}) \\\\
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{1}^{(i)} \\\\
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{2}^{(i)} \\\\
\cdots \\\\
\sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{n}^{(i)} \\\\
\end{pmatrix}
$$

* 向量化运算
  $$
  \nabla{J(\theta)}=\frac{2}{m}.\begin{pmatrix}
  \sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{0}^{(i)} \\\\
  \sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{1}^{(i)} \\\\
  \sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{2}^{(i)} \\\\
  \cdots \\\\
  \sum_{i=1}^m(X_{b}^{(i)}\theta-y^{(i)}).X_{n}^{(i)} \\\\
  \end{pmatrix} = \\\\
  \frac{2}{m}.(X_{b}^{(1)}\theta-y^{(1)},X_{b}^{(2)}\theta-y^{(2)},X_{b}^{(3)}\theta-y^{(3)},...,X_{b}^{(m)}\theta-y^{(m)}) .\begin{pmatrix}
  X_{0}^{(1)} & X_{1}^{(1)} & X_{2}^{(1)} & \cdots & X_{n}^{(1)} \\\\
  X_{0}^{(2)} & X_{1}^{(2)} & X_{2}^{(2)} & \cdots & X_{n}^{(2)} \\\\
  X_{0}^{(3)} & X_{1}^{(3)} & X_{2}^{(3)} & \cdots & X_{n}^{(3)} \\\\
  \cdots \\\\
  X_{0}^{(m)} & X_{1}^{(m)} & X_{2}^{(m)} & \cdots  & X_{n}^{(m)}
  \end{pmatrix} \\\\
  = \frac{2}{m}.(X_{b}\theta-y)^T.X_{b} = \frac{2}{m}.X_{b}^T.(X_{b}\theta-y)
  $$
  ​

  ​