## 超参数

#### 超参数和模型参数

* 超参数：在算法运行前需要决定的参数
* 模型参数：算法过程中学习的参数

> kNN算法中没有模型参数
>
> kNN算法中的k是典型的超参数

#### 寻找好的超参数

* 领域知识
* 经验数值
* 实验搜索

#### k近邻算法

* 曼哈顿距离

$$
\sum_{i=1}^n{|X_{i}^{(a)} - X_{i}^{(b)}|}
$$



* 明可夫斯基距离

$$
(\sum_{i=1}^n|X_{i}^{(a)} - X_{i}^{(b)}|^p)^{\frac{1}{p}}
$$

> 当p = 1时，明可夫斯基距离==曼哈顿距离
>
> 当p = 2时，明可夫斯基距离==欧拉距离

#### 更多的距离定义

* 向量空间余弦相似度 Cosine Similarity
* 调整余弦相似度 Adjusted Cosine Similarity
* 皮尔森相关系数 Pearson Correlation Coefficient
* Jaccard相似系数 Jaccard Coefficient

#### 数据归一化Feature Scaling

* 解决方案： 将所有的数据映射到同一尺度

* 最值归一化：把所有数据映射到0-1之间 normalization 

  > 适用于分布有明显边界的情况；受outlier影响较大

$$
x_{scale} = \frac{x - x_{min}}{x_{max} - x_{min}}
$$

* 均值方差归一化 把所有数据归一到均值为0方差为1的分布中  standardization

> 数据分布没有明显的边界；有可能存在极端数据值

$$
x_{scale} = \frac{x - x_{mean}}{S}
$$

#### 对测试数据集如何归一化?

测试数据是模拟真实环境

* 真实环境很有可能无法得到所有测试数据的均值和方差
* 对数据的归一化也是算法的一部分

要保存训练数据集得到的均值和方差

scikit-learn中使用Scaler

```python
import numpy as np

class StandardScaler:
    def __init__(self):
        self.mean_ = None
        self.scale_ = None
        
    def fit(self, X):
        """根据训练数据集X获得数据的均值和方差"""
        assert X.ndim == 2, "The dimension of X must be 2"
        self.mean_ = np.array([np.mean(X[:,i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:, i]) for i in range(X.shape[1])])
        return self
    def transform(self, X):
        """将X根据这个StandardScaler进行均值方差归一化处理"""
        assert X.ndim == 2, "The dimension of X must be 2"
        assert self.mean_ is not None and self.scale_ is not None, "must fit before transform"
        
        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:, col] = (X[:,col] - self.mean_[col]) / self.scale_[col]
            return resX
```



#### k近邻算法缺点

* 最大的缺点：效率低下

  > 如果训练集有m个样本，n个特征，则预测每一个新的数据，需要O(m*n)
  >
  > 优化，使用树结构： KD-Tree, Ball-Tree

* 高度数据相关

* 预测结构不具有可解释性

* 维数灾难

  > 随着维度的增加，看似相近的两个点直接的距离越来越大