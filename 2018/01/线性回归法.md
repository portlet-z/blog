## 线性回归法(Linear Regression)

* 解决回归问题
* 思想简单，容易实现
* 许多强大的非线性模型的基础
* 结果具有很好的可解释性
* 蕴含机器学习中的很多重要思想

 假设我们找到了最佳拟合的直线方程:$y = ax + b$, 则对于每一个样本点$x^{(i)}$,根据我们的直线方程，预测值为：$\hat{y}^{(i)} = ax^{(i)} + b$,真值为：$y^{(i)}$ .

我们希望$y^{(i)}$和$\hat{y}^{(i)}$的差距尽量小。表达$y^{(i)}$和$\hat{y}^{(i)}$的差距：$|y^{(i)} - \hat{y}^{(i)}|$ 或者 $(y^{(i)} - \hat{y}^{(i)})^2$

考虑所有样本：$\sum_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2$  ==>
$$
\hat{y}^{(i)} = ax^{(i)} + b
$$

$$
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2
$$
目标：找到a和b，使得$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2$尽可能小

$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2$ ==> 损失函数(loss function)  效用函数(utility function)

通过分析问题，确定问题的损失函数或者效用函数；通过最优化损失函数或者效用函数，获得机器学习的模型。

近乎所有参数学习算法都是这样的套路

典型的最小二乘法问题：最小化误差的平方
$$
a = \frac{\sum_{i=1}^m(x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^m(x^{(i)} - \bar{x})^2} \qquad b =\bar{y} - a\bar{x}
$$

#### 推导过程

$$
J(a,b) = \sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2 \qquad \frac{\partial{J(a,b)}}{\partial{a}} = 0 \qquad \frac{\partial{J(a,b)}}{\partial{b}} = 0
$$

$$
\frac{\partial{J(a,b)}}{\partial{b}} = \sum_{i=1}^m2(y^{(i)} -ax^{(i)} -b)(-1) = 0 \\
\sum_{i=1}^m(y^{(i)} -ax^{(i)} -b) = 0 \\
\sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} - \sum_{i=1}^mb=0  \longrightarrow \sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} -mb=0 \\
mb = \sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} \longrightarrow b = \bar{y} - a\bar{x}
$$

由上面推导出$b=\bar{y}-a\bar{x}$
$$
\frac{\partial{J(a,b)}}{\partial{a}} = \sum_{i=1}^m2(y^{(i)} - ax^{(i)} -b)(-x^{(i)}) = 0 \\
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)x^{(i)} = 0
$$


将$b=\bar{y}-a\bar{x}$带入$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)x^{(i)} = 0$推导出
$$
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -\bar{y}+a\bar{x})x^{(i)} = 0  \\
\sum_{i=1}^m(x^{(i)}y^{(i)} - a(x^{(i)})^2 - x^{(i)}\bar{y} + a\bar{x}x^{(i)}) =  \\
\sum_{i=1}^m(x^{(i)}y^{(i)}- x^{(i)}\bar{y} - a(x^{(i)})^2  + a\bar{x}x^{(i)}) = \\
\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y}) - \sum_{i=1}^m(a(x^{(i)})^2 - a\bar{x}x^{(i)}) = \\
\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y}) - a\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)}) = 0 \longrightarrow \\
a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})}
$$
由公式中$a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})}$
$$
\sum_{i=1}^mx^{(i)}\bar{y} = \bar{y}\sum_{i=1}^mx^{(i)} = m\bar{y}.\bar{x} = \bar{x}\sum_{i=1}^my^{(i)} = \sum_{i=1}^m\bar{x}y^{(i)}
$$

$$
m\bar{y}.\bar{x} = \sum_{i=1}^m\bar{x}.\bar{y}
$$

$$
a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})} \\
= \frac{\sum_{i=1}^m(x^{(i)}y^{(i)}-x^{(i)}\bar{y}-\bar{x}y^{(i)}+\bar{x}.\bar{y})}{\sum_{i=1}^m((x^{(i)})^2-\bar{x}x^{(i)}-\bar{x}x^{(i)}+\bar{x}^2)} \\
= \frac{\sum_{i=1}^m(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}{\sum_{i=1}^m(x^{(i)}-\bar{x})^2}
$$

```python
import numpy as np

class SimpleLinearRegression1:

    def __init__(self):
        """初始化SimpleLinearRegression模型"""
        self.a_ = None
        self.b_ = None
        
    def fit(self, x_train, y_train):
        """根据训练数据集x_train,y_train训练Simple Linear Regression模型"""
        assert x_train.ndim = 1, "Simple Linear Regression can only solve single feature training data."
        assert len(x_train) == len(y_train), "the size of x_train must be equal to the size of y_train"
        
        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)
        
        num = 0.0
        d = 0.0
        for x,y in zip(x_train, y_train):
            num += (x-x.mean) * y (y - y_mean)
            d += (x_x_mean) ** 2
            
        self.a_ = num/d
        self.b_ = y_mean - self.a_ * x_mean
        
        return self
        
    def predict(self, x_predict):
        """给定带预测数据集x_predict,返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, "Simple Linear Regression can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, "must fit before predict!"
        
        return np.array([self._predict(x) for x in x_predict])
        
    def _predict(self, x_single):
        """给定单个待遇测数据x_single,返回x的预测结果值"""
        return self.a_ * x_single + self.b_
        
    def __repr__(self):
        return "SimpleLinearRegression1()"
```



#### 向量化运算

化简 a 的值
$$
a = \frac{\sum_{i=1}^m(x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^m(x^{(i)} - \bar{x})^2}
$$
化简过程
$$
\sum_{i=1}^mw^{(i)}.v^{(i)} \qquad w=(w^{(1)},w^{(2)},...,w^{(i)}) \qquad v=v(v^{(1)},v^{(2)},...,v^{(i)}) \\
\sum_{i=1}^mw^{(i)}.v^{(i)} = w.v \\
$$

* 向量化运算代码

```python
import numpy as np

class SimpleLinearRegression2:

    def __init__(self):
        """初始化SimpleLinearRegression模型"""
        self.a_ = None
        self.b_ = None
        
    def fit(self, x_train, y_train):
        """根据训练数据集x_train,y_train训练Simple Linear Regression模型"""
        assert x_train.ndim = 1, "Simple Linear Regression can only solve single feature training data."
        assert len(x_train) == len(y_train), "the size of x_train must be equal to the size of y_train"
        
        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)
        
        num = (x_train - x_mean).dot(y_train - y_mean)
        d = (x_train - x_mean).dot(x_train - x_mean)
            
        self.a_ = num/d
        self.b_ = y_mean - self.a_ * x_mean
        
        return self
        
    def predict(self, x_predict):
        """给定带预测数据集x_predict,返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, "Simple Linear Regression can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, "must fit before predict!"
        
        return np.array([self._predict(x) for x in x_predict])
        
    def _predict(self, x_single):
        """给定单个待遇测数据x_single,返回x的预测结果值"""
        return self.a_ * x_single + self.b_
        
    def __repr__(self):
        return "SimpleLinearRegression2()"
        
```



#### 线性回归算法的评测

* 均方误差 MSE(Mean Squared Error)

$$
\frac{1}{m}\sum_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2
$$

* 均方根误差 RMSE(Root Mean Squared Error)

$$
\sqrt{\frac{1}{m}\sum_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2} = \sqrt{MSE_{test}}
$$

* 平均绝对误差 MAE(Mean Absolute Error)

$$
\frac{1}{m}|y_{test}^{(i)} - \hat{y}_{test}^{(i)}|
$$

* R Squared

$$
R^2 = 1 - \frac{SS_{residual}}{SS_{total}}  \qquad SS_{residual}(Residual \,Sum\,of\,Squres) \qquad SS_{total}(Total\,Sum\,of\,Squares)
$$

$$

$$

$$

$$

$$
R^2 = 1-\frac{\sum_{i}(\hat{y}^{(i)} - y^{(i)})^2}{\sum_{i}(\bar{y}-y^{(i)})^2}
$$

1. R^2 <= 1
2. R^2越大越好。当我们的预测模型不犯任何错误时，值为1
3. 当我们的模型等于基准模型时，R^2为0
4. 如果R^2<0,说明我们学习到的模型还不如基准模型，此时，很有可能我们的数据不存在任何线性关系

$$
R^2 = 1-\frac{\sum_{i}(\hat{y}^{(i)} - y^{(i)})^2}{\sum_{i}(\bar{y}-y^{(i)})^2} \\
 = 1-\frac{\frac{\sum_{i}(\hat{y}^{(i)} - y^{(i)})^2}{m}}{\frac{\sum_{i}(\bar{y}-y^{(i)})^2}{m}} = 1 - \frac{MSE(\hat{y},y)}{Var(y)}
$$

