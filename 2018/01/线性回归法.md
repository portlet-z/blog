## 线性回归法(Linear Regression)

* 解决回归问题
* 思想简单，容易实现
* 许多强大的非线性模型的基础
* 结果具有很好的可解释性
* 蕴含机器学习中的很多重要思想

 假设我们找到了最佳拟合的直线方程:$y = ax + b$, 则对于每一个样本点$x^{(i)}$,根据我们的直线方程，预测值为：$\hat{y}^{(i)} = ax^{(i)} + b$,真值为：$y^{(i)}$ .

我们希望$y^{(i)}$和$\hat{y}^{(i)}$的差距尽量小。表达$y^{(i)}$和$\hat{y}^{(i)}$的差距：$|y^{(i)} - \hat{y}^{(i)}|$ 或者 $(y^{(i)} - \hat{y}^{(i)})^2$

考虑所有样本：$\sum_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2$  ==>
$$
\hat{y}^{(i)} = ax^{(i)} + b
$$

$$
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2
$$
目标：找到a和b，使得$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2$尽可能小

$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2$ ==> 损失函数(loss function)  效用函数(utility function)

通过分析问题，确定问题的损失函数或者效用函数；通过最优化损失函数或者效用函数，获得机器学习的模型。

近乎所有参数学习算法都是这样的套路

典型的最小二乘法问题：最小化误差的平方
$$
a = \frac{\sum_{i=1}^m(x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^m(x^{(i)} - \bar{x})^2} \qquad b =\bar{y} - a\bar{x}
$$

#### 推导过程

$$
J(a,b) = \sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2 \qquad \frac{\partial{J(a,b)}}{\partial{a}} = 0 \qquad \frac{\partial{J(a,b)}}{\partial{b}} = 0
$$

$$
\frac{\partial{J(a,b)}}{\partial{b}} = \sum_{i=1}^m2(y^{(i)} -ax^{(i)} -b)(-1) = 0 \\
\sum_{i=1}^m(y^{(i)} -ax^{(i)} -b) = 0 \\
\sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} - \sum_{i=1}^mb=0  \longrightarrow \sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} -mb=0 \\
mb = \sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} \longrightarrow b = \bar{y} - a\bar{x}
$$

由上面推导出$b=\bar{y}-a\bar{x}$
$$
\frac{\partial{J(a,b)}}{\partial{a}} = \sum_{i=1}^m2(y^{(i)} - ax^{(i)} -b)(-x^{(i)}) = 0 \\
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)x^{(i)} = 0
$$


将$b=\bar{y}-a\bar{x}$带入$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)x^{(i)} = 0$推导出
$$
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -\bar{y}+a\bar{x})x^{(i)} = 0  \\
\sum_{i=1}^m(x^{(i)}y^{(i)} - a(x^{(i)})^2 - x^{(i)}\bar{y} + a\bar{x}x^{(i)}) =  \\
\sum_{i=1}^m(x^{(i)}y^{(i)}- x^{(i)}\bar{y} - a(x^{(i)})^2  + a\bar{x}x^{(i)}) = \\
\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y}) - \sum_{i=1}^m(a(x^{(i)})^2 - a\bar{x}x^{(i)}) = \\
\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y}) - a\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)}) = 0 \longrightarrow \\
a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})}
$$
由公式中$a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})}$
$$
\sum_{i=1}^mx^{(i)}\bar{y} = \bar{y}\sum_{i=1}^mx^{(i)} = m\bar{y}.\bar{x} = \bar{x}\sum_{i=1}^my^{(i)} = \sum_{i=1}^m\bar{x}y^{(i)}
$$

$$
m\bar{y}.\bar{x} = \sum_{i=1}^m\bar{x}.\bar{y}
$$

$$
a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})} \\
= \frac{\sum_{i=1}^m(x^{(i)}y^{(i)}-x^{(i)}\bar{y}-\bar{x}y^{(i)}+\bar{x}.\bar{y})}{\sum_{i=1}^m((x^{(i)})^2-\bar{x}x^{(i)}-\bar{x}x^{(i)}+\bar{x}^2)} \\
= \frac{\sum_{i=1}^m(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}{\sum_{i=1}^m(x^{(i)}-\bar{x})^2}
$$

```python
import numpy as np

class SimpleLinearRegression1:
    
    def __init__(self):
        """初始化Simple Linear Regression模型"""
        
```



