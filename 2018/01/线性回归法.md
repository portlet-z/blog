## 线性回归法(Linear Regression)

* 解决回归问题
* 思想简单，容易实现
* 许多强大的非线性模型的基础
* 结果具有很好的可解释性
* 蕴含机器学习中的很多重要思想

 假设我们找到了最佳拟合的直线方程:$y = ax + b$, 则对于每一个样本点$x^{(i)}$,根据我们的直线方程，预测值为：$\hat{y}^{(i)} = ax^{(i)} + b$,真值为：$y^{(i)}$ .

我们希望$y^{(i)}$和$\hat{y}^{(i)}$的差距尽量小。表达$y^{(i)}$和$\hat{y}^{(i)}$的差距：$|y^{(i)} - \hat{y}^{(i)}|$ 或者 $(y^{(i)} - \hat{y}^{(i)})^2$

考虑所有样本：$\sum_{i=1}^m(y^{(i)} - \hat{y}^{(i)})^2$  ==>
$$
\hat{y}^{(i)} = ax^{(i)} + b
$$

$$
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2
$$
目标：找到a和b，使得$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2$尽可能小

$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2$ ==> 损失函数(loss function)  效用函数(utility function)

通过分析问题，确定问题的损失函数或者效用函数；通过最优化损失函数或者效用函数，获得机器学习的模型。

近乎所有参数学习算法都是这样的套路

典型的最小二乘法问题：最小化误差的平方
$$
a = \frac{\sum_{i=1}^m(x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^m(x^{(i)} - \bar{x})^2} \qquad b =\bar{y} - a\bar{x}
$$

#### 推导过程

$$
J(a,b) = \sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)^2 \qquad \frac{\partial{J(a,b)}}{\partial{a}} = 0 \qquad \frac{\partial{J(a,b)}}{\partial{b}} = 0
$$

$$
\frac{\partial{J(a,b)}}{\partial{b}} = \sum_{i=1}^m2(y^{(i)} -ax^{(i)} -b)(-1) = 0 \\\\
\sum_{i=1}^m(y^{(i)} -ax^{(i)} -b) = 0 \\\\
\sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} - \sum_{i=1}^mb=0  \longrightarrow \sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} -mb=0 \\\\
mb = \sum_{i=1}^my^{(i)} - a\sum_{i=1}^mx^{(i)} \longrightarrow b = \bar{y} - a\bar{x}
$$

由上面推导出$b=\bar{y}-a\bar{x}$
$$
\frac{\partial{J(a,b)}}{\partial{a}} = \sum_{i=1}^m2(y^{(i)} - ax^{(i)} -b)(-x^{(i)}) = 0 \\\\
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)x^{(i)} = 0
$$


将$b=\bar{y}-a\bar{x}$带入$\sum_{i=1}^m(y^{(i)} - ax^{(i)} -b)x^{(i)} = 0$推导出
$$
\sum_{i=1}^m(y^{(i)} - ax^{(i)} -\bar{y}+a\bar{x})x^{(i)} = 0  \\\\
\sum_{i=1}^m(x^{(i)}y^{(i)} - a(x^{(i)})^2 - x^{(i)}\bar{y} + a\bar{x}x^{(i)}) =  \\\\
\sum_{i=1}^m(x^{(i)}y^{(i)}- x^{(i)}\bar{y} - a(x^{(i)})^2  + a\bar{x}x^{(i)}) = \\\\
\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y}) - \sum_{i=1}^m(a(x^{(i)})^2 - a\bar{x}x^{(i)}) = \\\\
\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y}) - a\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)}) = 0 \longrightarrow \\\\
a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})}
$$
由公式中$a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})}$
$$
\sum_{i=1}^mx^{(i)}\bar{y} = \bar{y}\sum_{i=1}^mx^{(i)} = m\bar{y}.\bar{x} = \bar{x}\sum_{i=1}^my^{(i)} = \sum_{i=1}^m\bar{x}y^{(i)}
$$

$$
m\bar{y}.\bar{x} = \sum_{i=1}^m\bar{x}.\bar{y}
$$

$$
a = \frac{\sum_{i=1}^m(x^{(i)}y^{(i)} - x^{(i)}\bar{y})}{\sum_{i=1}^m((x^{(i)})^2 - \bar{x}x^{(i)})} \\\\
= \frac{\sum_{i=1}^m(x^{(i)}y^{(i)}-x^{(i)}\bar{y}-\bar{x}y^{(i)}+\bar{x}.\bar{y})}{\sum_{i=1}^m((x^{(i)})^2-\bar{x}x^{(i)}-\bar{x}x^{(i)}+\bar{x}^2)} \\\\
= \frac{\sum_{i=1}^m(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}{\sum_{i=1}^m(x^{(i)}-\bar{x})^2}
$$

```python
import numpy as np

class SimpleLinearRegression1:

    def __init__(self):
        """初始化SimpleLinearRegression模型"""
        self.a_ = None
        self.b_ = None
        
    def fit(self, x_train, y_train):
        """根据训练数据集x_train,y_train训练Simple Linear Regression模型"""
        assert x_train.ndim = 1, "Simple Linear Regression can only solve single feature training data."
        assert len(x_train) == len(y_train), "the size of x_train must be equal to the size of y_train"
        
        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)
        
        num = 0.0
        d = 0.0
        for x,y in zip(x_train, y_train):
            num += (x-x.mean) * y (y - y_mean)
            d += (x_x_mean) ** 2
            
        self.a_ = num/d
        self.b_ = y_mean - self.a_ * x_mean
        
        return self
        
    def predict(self, x_predict):
        """给定带预测数据集x_predict,返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, "Simple Linear Regression can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, "must fit before predict!"
        
        return np.array([self._predict(x) for x in x_predict])
        
    def _predict(self, x_single):
        """给定单个待遇测数据x_single,返回x的预测结果值"""
        return self.a_ * x_single + self.b_
        
    def __repr__(self):
        return "SimpleLinearRegression1()"
```



#### 向量化运算

化简 a 的值
$$
a = \frac{\sum_{i=1}^m(x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^m(x^{(i)} - \bar{x})^2}
$$
化简过程
$$
\sum_{i=1}^mw^{(i)}.v^{(i)} \qquad w=(w^{(1)},w^{(2)},...,w^{(i)}) \qquad v=v(v^{(1)},v^{(2)},...,v^{(i)}) \\\\
\sum_{i=1}^mw^{(i)}.v^{(i)} = w.v \\
$$

* 向量化运算代码

```python
import numpy as np

class SimpleLinearRegression2:

    def __init__(self):
        """初始化SimpleLinearRegression模型"""
        self.a_ = None
        self.b_ = None
        
    def fit(self, x_train, y_train):
        """根据训练数据集x_train,y_train训练Simple Linear Regression模型"""
        assert x_train.ndim = 1, "Simple Linear Regression can only solve single feature training data."
        assert len(x_train) == len(y_train), "the size of x_train must be equal to the size of y_train"
        
        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)
        
        num = (x_train - x_mean).dot(y_train - y_mean)
        d = (x_train - x_mean).dot(x_train - x_mean)
            
        self.a_ = num/d
        self.b_ = y_mean - self.a_ * x_mean
        
        return self
        
    def predict(self, x_predict):
        """给定带预测数据集x_predict,返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, "Simple Linear Regression can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, "must fit before predict!"
        
        return np.array([self._predict(x) for x in x_predict])
        
    def _predict(self, x_single):
        """给定单个待遇测数据x_single,返回x的预测结果值"""
        return self.a_ * x_single + self.b_
        
    def __repr__(self):
        return "SimpleLinearRegression2()"
        
```



#### 线性回归算法的评测

* 均方误差 MSE(Mean Squared Error)

$$
\frac{1}{m}\sum_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2
$$

* 均方根误差 RMSE(Root Mean Squared Error)

$$
\sqrt{\frac{1}{m}\sum_{i=1}^m(y_{test}^{(i)} - \hat{y}_{test}^{(i)})^2} = \sqrt{MSE_{test}}
$$

* 平均绝对误差 MAE(Mean Absolute Error)

$$
\frac{1}{m}|y_{test}^{(i)} - \hat{y}_{test}^{(i)}|
$$

* R Squared

$$
R^2 = 1 - \frac{SS_{residual}}{SS_{total}}  \qquad SS_{residual}(Residual \,Sum\,of\,Squres) \qquad SS_{total}(Total\,Sum\,of\,Squares)
$$

$$
R^2 = 1-\frac{\sum_{i}(\hat{y}^{(i)} - y^{(i)})^2}{\sum_{i}(\bar{y}-y^{(i)})^2}
$$

1. R^2 <= 1
2. R^2越大越好。当我们的预测模型不犯任何错误时，值为1
3. 当我们的模型等于基准模型时，R^2为0
4. 如果R^2<0,说明我们学习到的模型还不如基准模型，此时，很有可能我们的数据不存在任何线性关系

$$
R^2 = 1-\frac{\sum_{i}(\hat{y}^{(i)} - y^{(i)})^2}{\sum_{i}(\bar{y}-y^{(i)})^2} \\\\
 = 1-\frac{\frac{\sum_{i}(\hat{y}^{(i)} - y^{(i)})^2}{m}}{\frac{\sum_{i}(\bar{y}-y^{(i)})^2}{m}} = 1 - \frac{MSE(\hat{y},y)}{Var(y)}
$$



### 多元线性回归

$$
\hat{y}^{(i)}=\theta_{0}+\theta_{1}X_{1}^{(i)}+\theta_{2}X_{2}^{(i)}+...+\theta_{n}X_{n}^{(i)}
$$

目标：找到$\theta_{0},\theta_{1},\theta_{2},…,\theta_{n}$,使得$\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$尽可能小
$$
\theta=(\theta_{0},\theta_{1},\theta_{2},...,\theta_{n})^T \\\\
\hat{y}^{(i)}=\theta_{0}X_{0}^{(i)}+\theta_{1}X_{1}^{(i)}+\theta_{2}X_{2}^{(i)}+...+\theta_{n}X_{n}^{(i)},X_{0}^{(i)}\equiv1 \\\\
X^{(i)}=(X_{0}^{(i)},X_{1}^{(i)},X_{2}^{(i)},...,X_{n}^{(i)}) \\\\
\hat{y}^{(i)}=X^{(i)}.\theta
$$

$$
X_{b}=\begin{pmatrix}
1 & X_{1}^{(1)} & X_{2}^{(1)} & \cdots & X_{n}^{(1)} \\\\
1 & X_{1}^{(2)} & X_{2}^{(2)} & \cdots & X_{n}^{(2)} \\\\
\cdots & & & & \cdots \\\\
1 & X_{1}^{(m)} & X_{2}^{(m)} & \cdots & X_{n}^{(m)}
\end{pmatrix} \qquad
\theta=\begin{pmatrix}
\theta_{0} \\\\
\theta_{1} \\\\
\theta_{2} \\\\
\cdots \\\\
\theta_{n}
\end{pmatrix}
$$

$$
\hat{y}=X_{b}.\theta
$$

$$
\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2=(y-X_{b}.\theta)^T(y-X_{b}.\theta) \rightarrow \theta= (X_{b}^TX_{b})^{-1}X_{b}^Ty
$$

* 多元线性回归的正规方程解(Normal Equation) $\theta= (X_{b}^TX_{b})^{-1}X_{b}^Ty$
  - 问题：时间复杂度高：O(n^3)
  - 优点：不需要对数据做归一化处理

```python
import numpy as np
from .metrics import r2_score

class LinearRegression:
    
    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.iterception_ = None
        self._theta = None
        
    def fit_normal(self, X_train, y_train):
        """根据训练数据集X_train, y_train训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], "the size of X_train must be equal to the size of y_train"
        X_b = np.hstack([np.ones((len(X_train), 1)),X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self
    
    def predict(self, X_predict):
        """给定待遇测数据集X_predict,返回表示X_predict的结果向量"""
        assert self.interception_ is not None and self.coef_ is not None, "must fit before predict"
        assert X_predict.shape[1] == len(self.coef_), "the feature number of X_predict must be equal to X_train"
        X_b = np.hstack([np.ones((len(X_predict), 1)),X_predict])
        return X_b.dot(self._theta)
    
    def score(self, X_test, y_test):
        """根据测试数据集X_test和y_test确定当前模型的准确度"""
        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)
        
    def __repr__(self):
        return "LinearRegression()"
```

#### 线性回归算法总结

* 典型的参数学习
  - 对比kNN: 非参数学习
* 只能解决回归问题
  - 虽然很多分类方法中，线性回归是基础（如逻辑回归）
  - 对比kNN: 既可以解决分类问题，又可以解决回归问题
* 对数据有假设：线性
  - 对比kNN对数据没有假设
* 优点：对数据具有强解释性