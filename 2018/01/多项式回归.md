## 多项式回归

* PolynomialFeatures(degree=2)

$$
x_1,x_2 \rightarrow 1,x_1,x_2,x_1^2,x_1x_2,x_2^2
$$

* PolymialFeatures(degree=3)

$$
x_1,x_2 \rightarrow 1,x_1,x_2 \qquad x_1^2,x_2^2,x_1x_2 \qquad x_1^3,x_2^3,x_1^2x_2,x_1x_2^2
$$

#### 学习曲线

* 随着训练样本的逐渐增多，算法训练出的模型的表现能力

#### 测试数据集的意义

* 验证数据：调整超参数使用的数据集
* 测试数据：作为衡量最终模型性能的数据集
* 交叉验证(Cross Validation)：k个模型的均值作为结果调参 

#### k-folds 交叉验证

* 把训练数据集分成k份，称为k-folds cross validation
* 缺点：每次训练k个模型，想当于整体性能慢了k倍

#### 留一法 LOO-CV

* 把训练数据集分成m份，称为留一法 Leave-One-Out Cross Validation
* 完全不受随机的影响，最接近模型真正的性能指标
* 缺点：计算量巨大

#### 偏差方差权衡 Bias Variance Trade off

* 模型误差 = 偏差(Bias) + 方差(Variance) + 不可避免的误差
* 导致偏差的主要原因：
  * 对问题本身的假设不正确：如：非线性数据使用线性回归
  * 欠拟合 underfitting
* 方差(Variance):
  * 数据的一点点扰动都会较大的影响模型。
  * 通常原因：使用的模型太复杂。如高阶多项式回归。
  * 过拟合 overfitting
* 偏差和方差
  * 有一些算法天生是高方差的算法。 kNN
  * 非参数学习通常都是高方差算法。因为不对数据进行任何假设
  * 有一些算法天生是高偏差算法。线性回归
  * 参数学习通常都是高偏差算法。因为对数据具有极强的假设
  * 大多数算法具有相应的参数，可以调整偏差和方差 如：kNN中的k, 线性回归中使用多项式
  * 偏差和方差通常是矛盾的。降低偏差，会提高方差。降低方差，会提高偏差。
  * 机器学习的主要挑战来自于方差。
  * 解决高方差的通常手段：
    * 降低模型的复杂度
    * 减少数据维度：降噪
    * 增加样本数
    * 使用验证集
    * 模型的正则化

#### 模型泛化与岭回归

* 模型正则化(Regularization):限制参数的大小
  * 目标：使$\sum_{i=1}^m(y^{(i)}-\theta_0-\theta_1X_1^{(i)}-\theta_2X_2^{(i)}-\cdots-\theta_nX_n^{(i)})^2$尽可能小 ==> 使$J(\theta)=MSE(y,\hat{y};\theta)$尽可能小
  * 加入模型正则化，目标：使$J(\theta)=MSE(y,\hat{y};\theta)+\alpha\frac{1}{2}\sum_{i=1}^n\theta_i^2$尽可能小（岭回归 Ride Regression）

#### LASSO Regression (Least Absolute Shrinkage and Selection Operator Regression)

* 目标：使$J(\theta)=MSE(y,\hat{y};\theta)+\alpha\sum_{i=1}^n|\theta_i|$尽可能小
* LASSO趋向于使得一部分theta值变为0.所以可作为特征选择用

#### L1,L2和弹性网络

* Ridge $\frac{1}{2}\alpha\sum_{i=1}^n\theta_i^2$   MSE$\frac{1}{m}\sum_{i=1}^n(y_i-\hat{y}_i)^2$  欧拉距离 $\sqrt{\sum_{i=1}^n(x_i^{(1)}-x_i^{(2)})^2}$
* LASSO $\sum_{i=1}^n|\theta_i|$  MAE $\frac{1}{n}\sum_{i=1}^n|y_i-\hat{y}_i|$  曼哈顿距离 $\sum_{i=1}^n|x_i^{(1)}-x_i^{(2)}|$
* 明可夫斯基距离(Minkowski Distance) $(\sum_{i=1}^n|X_i^{(a)}-X_i^{(b)}|^p)^\frac{1}{p}$   Lp范数$||x||_p=(\sum_{i=1}^n|x_i|^p)^\frac{1}{p}$

#### 弹性网 Elastic Net

$$
J(\theta)=MSE(y,\hat{y;\theta}) + r\alpha\sum_{i=1}^n|\theta_i| + \frac{1-r}{2}\alpha\sum_{i=1}^n\theta_i^2
$$

